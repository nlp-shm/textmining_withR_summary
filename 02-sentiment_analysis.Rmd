# Sentiment Analysis {#SA}

## Chapter Take Away


:::note

__Take away__

* We use inner_join to find sentiment in the work
* we find a sentiment score for each word using the lexicon and inner_join()

:::

##  Sentiment Analysis


```{r message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
library(janeaustenr)
library(lexicon)
```


![Lexicon-based Sentiment Analysis](/Users/shamsuddeen/Documents/R Directory/BookDown/Textmining_with_R/images/SA.png)




## 2.1 The sentiments dataset {-}

The tidytext package provides access to several sentiment lexicons. Three general-purpose lexicons wich are based on unigrams are: 


* AFINN from Finn Årup Nielsen:  assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.


* bing from Bing Liu and collaborators : categorizes words in a binary fashion into positive and negative categories. 



* nrc from Saif Mohammad and Peter Turney. :  categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise and trust.


The function `get_sentiments()` is used to load a specific lexicon:

```{r}
get_sentiments("afinn") # load affin lexicon
```

```{r}
get_sentiments("bing")

```

```{r}
##get_sentiments("nrc") Seeme not working at the moment

```

## Lexicon package not in the book


[Lexicon package](https://cran.r-project.org/web/packages/lexicon/lexicon.pdf) Contains a lot of available lexicons that are used in text analysis. It can also be found on github repo [here](https://github.com/trinker/lexicon).



```{r}
library(lexicon)
```


There are many functions in the lexicon package that can allow use to load the data. Example, available_data(regex), has many as shwon below:

* `available_data()`
* `available_data('hash_')`
* `available_data('hash_sentiment')`
* `available_data('python')`
* `available_data('English')`
* `available_data('Stopword')`
* `available_data('prof')`



```{r}
available_data()
```

Loading available data using `data` and name of the data
```{r}

data(profanity_racist) # now the data profanity_racist is loaded
```


```{r}
head(profanity_racist)
```


:::note
__How lexicon is validate__

How were these sentiment lexicons put together and validated? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data.
:::

General purpose Sentiment lexicon may offer less accuracy than domain specific lexicon.  In that case,  domain-specific sentiment lexicons available, constructed to be used with text from a specific content area. We will explores an analysis using a sentiment lexicon specifically for finance in this chapter.

:::note
Dictionary method do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only. For many kinds of text (like the narrative examples), there are not sustained sections of sarcasm or negated text, so this is not an important effect. Also, we can use a tidy text approach to begin to understand what kinds of negation words are important in a given text; see example in chapter 9
:::


## 2.2 Sentiment analysis with inner join {-}



With data in a tidy format, sentiment analysis can be done as an inner join.  We remove stop words using `antijoin` operation, and perform sentiment analysis with  `inner join` operation.



```{r}

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

> Returns a vector whose elements are the cumulative sums, products, minima or maxima of the elements of the argument.(cumsum(x), cumprod(x), cummax(x),cummin(x))


Let us use `nrc` emotion lexicon to find emotion related to `joy` in Emma only book. But, we are going to use the `lexicon` package to extaract the data. Th data from `tidytext` is givin me error and I cannot get it. 

```{r}
data("nrc_emotions")
```
```{r}
nrc_emotions_joy <- nrc_emotions %>% 
                select(term, joy) %>% 
                filter(joy != 0)
                  
nrc_emotions_joy
```

Renaming the word `term` to `word` because if we want use innerjoint, the two table must have unique key to perform the joint.

```{r}
nrc_emotions_joy <- rename(nrc_emotions_joy, word = term) %>% 
  mutate(sentiment = case_when(joy > 0  ~  "joy")) %>% 
  select(-joy)
```

```{r}
nrc_emotions_joy 
```


```{r}
tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_emotions_joy) %>%
  count(word, sort = TRUE)
```

From above, we can see that positive words such as : hope, friendship, and love here. We also see some words that may not be used joyfully by Austen (“found”, “present”);

continue from below:
>We can also examine how sentiment changes throughout each novel. We can do this with just a handful of lines that are mostly dplyr functions. First, we find a sentiment score for each word using the Bing lexicon and inner_join().


:::note

Small sections of text may not have enough words in them to get a good estimate of sentiment while really large sections can wash out narrative structure. For these books, using 80 lines works well, but this can vary depending on individual texts, how long the lines were to start with, etc.

So, in the example below, we use the `%/%` operator to perform integer division (x %/% y is equivalent to floor(x/y)) so the index keeps track of which 80-line section of text we are counting up negative and positive sentiment in.

:::


```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  #spread(sentiment, n, fill = 0) #%>%  # fill specify the missing value to set if there is any, but spread is now replaced with `pivot_wider`
  pivot_wider(names_from = sentiment , values_from = n) %>% # I replaced with pivot_wider function
  mutate(sentiment = positive - negative)
jane_austen_sentiment
```



Now we can plot these sentiment scores across the plot trajectory of each novel. We will plotting against the index on the x-axis that keeps track of narrative time in sections of text.


```{r}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book))+
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol =2, scales = "free_x")
```

## 2.3 Comparing the three sentiment dictionaries {-}


In this section we will use three lexicons and examine how the sentiment changes across the narrative arc of Pride and Prejudice. First, let’s use filter() to choose only the words from the one novel we are interested in.

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice
```

AFFIN sentiment lexicon


```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")
afinn
```





Bin and nrc lexicon:


```{r}
bing_and_nrc <- bind_rows(pride_prejudice %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          pride_prejudice %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

> I canot run the example above because the NRC is giving an error.

But, the conclution is that , the three different lexicons for calculating sentiment give results that are different in an absolute sense but have similar relative trajectories through the novel

>NRC vs Bing:  Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon. This will contribute to the effect we see in the plot above, as will any systematic difference in word matches, e.g. if the negative words in the NRC lexicon do not match the words that Jane Austen uses very well. Whatever the source of these differences, we see similar relative trajectories across the narrative arc, with similar changes in slope, but marked differences in absolute sentiment from lexicon to lexicon. This is all important context to keep in mind when choosing a sentiment lexicon for analysis.

## 2.4 Most common positive and negative words {-}

What is most positive and negative in our lexicon: We can use `count()`  with arguments of both `word` and `sentiment`, we can find out how much each word contributed to each sentiment.


```{r}
```

count() lets you quickly count the unique values of one or more variables: df %>% count(a, b) is roughly equivalent to df %>% group_by(a, b) %>% summarise(n = n()). So, we can ungroup() after count.

```{r}
bing_word_count <- tidy_books %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
bing_words_count
```


Let us put this in ggplot:

```{r}
bing_word_count %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

```


Figure  above lets us spot an anomaly in the sentiment analysis; the word “miss” is coded as negative but it is used as a title for young, unmarried women in Jane Austen’s works. If it were appropriate for our purposes, we could easily add “miss” to a custom stop-words list using bind_rows(). We could implement that with a strategy such as this.


We can create tiblle as this
```{r}
tibble(word = c("miss"), lexicon = c("custom"))
```

So, we can perform bind rows:

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```
## 2.5 Wordclouds {-}


Let us use wordcloud package, which uses base R graphics to find the most common words in Jane Austen’s works as a whole again:


```{r}
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Other similar function like comparison.cloud(), you may need to turn the data frame into a matrix with `reshape2’s acast()`. Lets see how it is done


```{r}
library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
 
```

> The size of a word’s text  is in proportion to its frequency within its sentiment. We can use this visualization to see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.

## 2.6 Looking at units beyond just words {-}

Soemtimes, sentiment analysis is better done using not only unigram, but using other ngrams. For example, the text:I am not having a good day, is not positive sentence, but using unigram analysis will put this as positive sentence.

R packages included coreNLP (T. Arnold and Tilton 2016), cleanNLP (T. B. Arnold 2016), and sentimentr (Rinker 2017) are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences, and it makes sense to use a new name for the output column in such a case.

Lets look at example:


```{r}
PandP_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

PandP_sentences$sentence[4]
PandP_sentences$sentence[3]
PandP_sentences$sentence[2]


```


Another option in `unnest_tokens()` is to split into tokens using a regex pattern. We could use this, for example, to split the text of Jane Austen’s novels into a data frame by chapter.



```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```



 Now, we can use tidy text analysis to ask questions such as what are the most negative chapters in each of Jane Austen’s novels? First, let’s get the list of negative words from the Bing lexicon. Second, let’s make a data frame of how many words are in each chapter so we can normalize for the length of chapters. Then, let’s find the number of negative words in each chapter and divide by the total words in each chapter. For each book, which chapter has the highest proportion of negative words?

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

These are the chapters with the most sad words in each book, normalized for number of words in the chapter. What is happening in these chapters? In Chapter 43 of Sense and Sensibility Marianne is seriously ill, near death, and in Chapter 34 of Pride and Prejudice Mr. Darcy proposes for the first time (so badly!). Chapter 46 of Mansfield Park is almost the end, when everyone learns of Henry’s scandalous adultery, Chapter 15 of Emma is when horrifying Mr. Elton proposes, and in Chapter 21 of Northanger Abbey Catherine is deep in her Gothic faux fantasy of murder, etc. Chapter 4 of Persuasion is when the reader gets the full flashback of Anne refusing Captain Wentworth and how sad she was and what a terrible mistake she realized it to b