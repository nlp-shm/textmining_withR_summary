[
["ngrams.html", "Chapter 5 Relationships between words: n-grams and correlations Chapter Take Away 5.1 The tidy text format 4.1 Tokenizing by n-gram 4.1.1 Counting and filtering n-grams 4.1.3 Using bigrams to provide context in sentiment analysis", " Chapter 5 Relationships between words: n-grams and correlations Chapter Take Away Take away Term frequency: tf idf 5.1 The tidy text format library(tidytext) library(tidyverse) library(janeaustenr) Many text analysis problems want find the relationships between words, whether : examining which words tend to follow others immediately, or Which word tend to co-occurwithin the same documents. The chapter explore how to use tidytext for calculating and visualizing relationships between words in your text dataset. This includes the token = “ngrams” argument, which tokenizes by pairs of adjacent words rather than by individual ones The chapter also introduced two new packages: ggraph: which extends ggplot2 to construct network plots, and widyr which calculates pairwise correlations and distances within a tidy data frame. Together these expand our toolbox for exploring text within the tidy data framework. 4.1 Tokenizing by n-gram Lets find bigrams on asuten works: austen_bigrams &lt;- austen_books() %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) austen_bigrams ## # A tibble: 725,049 x 2 ## book bigram ## &lt;fct&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility sense and ## 2 Sense &amp; Sensibility and sensibility ## 3 Sense &amp; Sensibility sensibility by ## 4 Sense &amp; Sensibility by jane ## 5 Sense &amp; Sensibility jane austen ## 6 Sense &amp; Sensibility austen 1811 ## 7 Sense &amp; Sensibility 1811 chapter ## 8 Sense &amp; Sensibility chapter 1 ## 9 Sense &amp; Sensibility 1 the ## 10 Sense &amp; Sensibility the family ## # … with 725,039 more rows Bigrams can overlaf e.g “sense and” is one token, while “and sensibility” is another. 4.1.1 Counting and filtering n-grams Finding bigrams (n = 2) Anlysing most common bi-grams using count: austen_bigrams %&gt;% count(bigram, sort = TRUE ) ## # A tibble: 211,236 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 of the 3017 ## 2 to be 2787 ## 3 in the 2368 ## 4 it was 1781 ## 5 i am 1545 ## 6 she had 1472 ## 7 of her 1445 ## 8 to the 1387 ## 9 she was 1377 ## 10 had been 1299 ## # … with 211,226 more rows austen_bigrams ## # A tibble: 725,049 x 2 ## book bigram ## &lt;fct&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility sense and ## 2 Sense &amp; Sensibility and sensibility ## 3 Sense &amp; Sensibility sensibility by ## 4 Sense &amp; Sensibility by jane ## 5 Sense &amp; Sensibility jane austen ## 6 Sense &amp; Sensibility austen 1811 ## 7 Sense &amp; Sensibility 1811 chapter ## 8 Sense &amp; Sensibility chapter 1 ## 9 Sense &amp; Sensibility 1 the ## 10 Sense &amp; Sensibility the family ## # … with 725,039 more rows As you can see, still non intersting words appear at the top, which we called stop words. We can separate the bigram words into two columns word1 and word2 and check if a word is stop-words and then remove it. bigrams_separated &lt;- austen_bigrams %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) bigrams_filtered &lt;- bigrams_separated %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) # new bigram counts: bigram_counts &lt;- bigrams_filtered %&gt;% count(word1, word2, sort = TRUE) bigram_counts ## # A tibble: 33,421 x 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 sir thomas 287 ## 2 miss crawford 215 ## 3 captain wentworth 170 ## 4 miss woodhouse 162 ## 5 frank churchill 132 ## 6 lady russell 118 ## 7 lady bertram 114 ## 8 sir walter 113 ## 9 miss fairfax 109 ## 10 colonel brandon 108 ## # … with 33,411 more rows We can put together the bi-gram using function the unite(), which is the inverse of separate() Thus, “separate/filter/count/unite” let us find the most common bigrams not containing stop-words. bigrams_united &lt;- bigrams_filtered %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) bigrams_united ## # A tibble: 44,784 x 2 ## book bigram ## &lt;fct&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility jane austen ## 2 Sense &amp; Sensibility austen 1811 ## 3 Sense &amp; Sensibility 1811 chapter ## 4 Sense &amp; Sensibility chapter 1 ## 5 Sense &amp; Sensibility norland park ## 6 Sense &amp; Sensibility surrounding acquaintance ## 7 Sense &amp; Sensibility late owner ## 8 Sense &amp; Sensibility advanced age ## 9 Sense &amp; Sensibility constant companion ## 10 Sense &amp; Sensibility happened ten ## # … with 44,774 more rows Finding Trigam (n = 3) austen_books() %&gt;% unnest_tokens(trigram, text, token = &quot;ngrams&quot;, n = 3) %&gt;% separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word) %&gt;% count(word1, word2, word3, sort = TRUE) ## # A tibble: 8,757 x 4 ## word1 word2 word3 n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 dear miss woodhouse 23 ## 2 miss de bourgh 18 ## 3 lady catherine de 14 ## 4 catherine de bourgh 13 ## 5 poor miss taylor 11 ## 6 sir walter elliot 11 ## 7 ten thousand pounds 11 ## 8 dear sir thomas 10 ## 9 twenty thousand pounds 8 ## 10 replied miss crawford 7 ## # … with 8,747 more rows 4.1.1 Analyzing bigrams We can use bigrams to do some exploratory analysis. For example, finding the most common strees in each book bigrams_filtered %&gt;% filter(word2 == &quot;street&quot;) %&gt;% count(book, word1, sort = TRUE) # we used word1, because we have Oxford street always ## # A tibble: 34 x 3 ## book word1 n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility berkeley 16 ## 2 Sense &amp; Sensibility harley 16 ## 3 Northanger Abbey pulteney 14 ## 4 Northanger Abbey milsom 11 ## 5 Mansfield Park wimpole 10 ## 6 Pride &amp; Prejudice gracechurch 9 ## 7 Sense &amp; Sensibility conduit 6 ## 8 Sense &amp; Sensibility bond 5 ## 9 Persuasion milsom 5 ## 10 Persuasion rivers 4 ## # … with 24 more rows Using word2: bigrams_filtered %&gt;% filter(word2 == &quot;street&quot;) %&gt;% count(book, word2, sort = TRUE) # we used word1, because we have Oxford street always ## # A tibble: 6 x 3 ## book word2 n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility street 46 ## 2 Northanger Abbey street 31 ## 3 Persuasion street 20 ## 4 Mansfield Park street 14 ## 5 Pride &amp; Prejudice street 12 ## 6 Emma street 4 We can use bigram to also find tf-idf: bigram_tf_idf &lt;- bigrams_united %&gt;% count(book, bigram) %&gt;% bind_tf_idf(bigram, book, n) %&gt;% arrange(desc(tf_idf)) bigram_tf_idf ## # A tibble: 36,217 x 6 ## book bigram n tf idf tf_idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Persuasion captain wentworth 170 0.0299 1.79 0.0535 ## 2 Mansfield Park sir thomas 287 0.0287 1.79 0.0515 ## 3 Mansfield Park miss crawford 215 0.0215 1.79 0.0386 ## 4 Persuasion lady russell 118 0.0207 1.79 0.0371 ## 5 Persuasion sir walter 113 0.0198 1.79 0.0356 ## 6 Emma miss woodhouse 162 0.0170 1.79 0.0305 ## 7 Northanger Abbey miss tilney 82 0.0159 1.79 0.0286 ## 8 Sense &amp; Sensibility colonel brandon 108 0.0150 1.79 0.0269 ## 9 Emma frank churchill 132 0.0139 1.79 0.0248 ## 10 Pride &amp; Prejudice lady catherine 100 0.0138 1.79 0.0247 ## # … with 36,207 more rows There are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and may provide context that makes tokens more understandable (for example, “pulteney street”, in Northanger Abbey, is more informative than “pulteney”). However, the per-bigram counts are also sparser: a typical two-word pair is rarer than either of its component words. Thus, bigrams can be especially useful when you have a very large text dataset. 4.1.3 Using bigrams to provide context in sentiment analysis continue from here "]
]
