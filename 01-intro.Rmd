# Tidy text format {#tidytext}

## Chapter Take Away


:::note

__Take away__

* A tidy text format is a table with one-token-per-row
* Tidytext package allow us to perform tidy text analysis
* unnest_tokens(word, text) format:  word is name of the output column (word in this case) and text is the column where the data is coming from 
* `unnest_tokens()` converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior).
* the default tokenization in `unnest_tokens()` is for `single words`, but there is other such as ngrams,sentences, lines,  character_ shingles and regex. e.g  to tokenize into bi-grams: `unnest_tokens(ngram, txt, token = "ngrams", n = 2)`

* If the data is in  tidytext format( one-word-per-row), use `anti_join()` in combination with `stop words` to remove stop words from the text e.g `tidy_books <- tidy_books %>% anti_join(stop_words)`
* Find word frequncy using `count` e.g  using tidytext :`tidy_hgwells %>%count(word, sort = TRUE)`
:::

# The tidy text format 


```{r message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
```


## 1.1 Contrasting tidy text with other data structures {-}

A tidy text format is a table with one-token-per-row. Structuring text data in this way means that it conforms to tidy data principles and can be manipulated with a set of consistent tools.

Tidytext is different from the way other data structure is organized (e.g text, Corpus, and Document-term matrix).

## 1.2 The unnest_tokens function {-}


The unnest function allow us to 

Example: Given the text below, change it to tidytext:

```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
```


Before chaning `text` to tidytext format, we need to change it to dataframe first:


```{r}
text_df <- tibble(line = 1:4, text = text)
text_df
```

To chnage `text` to be use for  tidy text analysis, we need to convert this so that it has `one-token-per-document-per-row` using `unnest_tokens` functions as shown below:


:::note

__unnest_tokens___

Split a column into tokens using the tokenizers package, splitting the table into one-token-per-row. This function supports non-standard evaluation through the tidyeval framework.

:::

So, we change it to tidy text format below:

```{r}
text_df %>%
  unnest_tokens(word, text) # word is name of the output column (word in this case) and text is the column where the data is coming from (text_df, has column with the data)
```

Now, we’ve split each row so that there is one token (word) in each row of the new data frame; the default tokenization in `unnest_tokens()` is for `single words`, but there is other such as ngrams,sentences, lines,  character_ shingles and regex.


From tibble above, we can see that:

* Other columns, such as the line number each word came from, are retained.

* Punctuation has been stripped.

* By default, `unnest_tokens()` converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior).

:::note
Examples of using the unnest functions:


```{r}
library(janeaustenr)
```

```{r}
d <- tibble(txt = prideprejudice)
```

### Sentences 

```{r}
d %>%
  unnest_tokens(sentence, txt, token = "sentences")
```
### Word



```{r}
d %>%
  unnest_tokens(word, txt)
```


### Chapter


```{r}
d %>%
  unnest_tokens(chapter, txt, token = "regex", pattern = "Chapter [\\\\d]")
```

### By n-gram


```{r}
d %>%
  unnest_tokens(ngram, txt, token = "ngrams", n = 2)
```

### tri-gram

```{r}
d %>%
  unnest_tokens(ngram, txt, token = "ngrams", n = 3)
```
### character_ shingles : Defined character


```{r}
d %>%
  unnest_tokens(shingle, txt, token = "character_shingles", n = 4)
```

:::

![Tidytext concept](/Textmining_with_R/01-intro_files/tidytex1.png)


## 1.3 Tidying the works of Jane Austen {-}


```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

original_books
```


```{r}
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books
```

Now that the data is in one-word-per-row format, we can do some text analysis, such as removing `stop words`. We can remove the stop words using `anti_join()` as shown below:


```{r}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)
```


>The stop_words dataset in the `tidytext package` contains stop words from three lexicons. We can use them all together, as we have here, or filter() to only use one set of stop words if that is more appropriate for a certain analysis.

#### Counting most common words

We canuse dplyr’s count() to find the most common words in all the books as a whole.


```{r}
tidy_books %>% 
  count(word , sort =  TRUE)

```

Again, we can use ggplot here to plot the word distribution as obtain above:


```{r}
tidy_books %>% 
  count(word , sort =  TRUE) %>% 
  filter( n > 600) %>% 
  mutate(words = reorder(word , n)) %>% 
  ggplot(aes(word, n))+
  geom_col()+
  xlab(NULL) 

```


we can flip the cordinate using  `coord_flip()` to show the graph in better way:


```{r}
tidy_books %>% 
  count(word , sort =  TRUE) %>% 
  filter( n > 600) %>% 
  mutate(words = reorder(word , n)) %>% 
  ggplot(aes(word, n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()
 
```

## 1.4 The gutenbergr package {-}

The gutenbergr package provides access to the public domain works from the Project Gutenberg collection. The package includes tools both for downloading books (stripping out the unhelpful header/footer information), and a complete dataset of Project Gutenberg metadata that can be used to find works of interest.

The function `gutenberg_download()`  downloads one or more works from Project Gutenberg by ID. Other functions are available that you can explore metadata, pair Gutenberg ID with title, author, language, etc., or gather information about authors.


## 1.5 Word frequencies {-}

A common task in text mining is to look at word frequencie. We will explore that is this section using science fiction and fantasy novels by H.G. Wells, who lived in the late 19th and early 20th centuries.

Lets start with the book  these four books: `The Time Machine, The War of the Worlds, The Invisible Man, and The Island of Doctor Moreau` and download them using the  `gutenberg_download()` and the Project Gutenberg ID numbers for each novel as shown below:


```{r}
library(gutenbergr)

hgwells <- gutenberg_download(c(35, 36, 5230, 159))
```

Tidytexting and removing stop words
```{r}
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```

Most common words

```{r}

tidy_hgwells %>% 
  count(word, sort = TRUE)
```
Another works:


```{r}
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
```

```{r}
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
```
```{r}
tidy_bronte %>%
  count(word, sort = TRUE)
```



Calculating the frequency for each word for the works of Jane Austen, the Brontë sisters, and H.G. Wells by binding the data frames together. 

```{r}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)
```

We use str_extract() here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (like italics). The tokenizer treated these as words, but we don’t want to count “_any_” separately from “any” as we saw in our initial data exploration before choosing to use str_extract()


```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```


TODO:

Come back and re-read again the section from `1.5 Word frequencies`
