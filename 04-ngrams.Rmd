# Relationships between words: n-grams and correlations {#ngrams}

## Chapter Take Away {-}


:::note

__Take away__

* Term frequency:

* tf

* idf
:::

## The tidy text format 


```{r message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
library(janeaustenr)

```




Many text analysis problems want find the relationships between words, whether :

* examining which words tend to `follow others immediately`, or 
* Which word tend to `co-occur `within the same documents.



>The chapter explore how to use tidytext for calculating and visualizing relationships between words in your text dataset. This includes the token = "ngrams" argument, which tokenizes by pairs of adjacent words rather than by individual ones


The chapter also introduced two new packages:

* **ggraph:**  which extends ggplot2 to construct network plots, and 
* **widyr** which calculates pairwise correlations and distances within a tidy data frame. Together these expand our toolbox for exploring text within the tidy data framework.



## 4.1 Tokenizing by n-gram {-}

Lets find bigrams on asuten works:

```{r}
austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams
```

> Bigrams can overlaf e.g “sense and” is one token, while “and sensibility” is another.

## 4.1.1 Counting and filtering n-grams {-}


### Finding bigrams (n = 2) {-}
Anlysing most common bi-grams using count:

```{r}
austen_bigrams %>% 
  count(bigram, sort = TRUE )

austen_bigrams
```



As you can see, still non intersting words appear at the top, which we called stop words. We can separate the bigram words into two columns word1 and word2 and check if a word is stop-words and then remove it.


```{r}

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts


```

We can put together the bi-gram using function the `unite()`, which is the inverse of `separate()`


:::note

Thus, “separate/filter/count/unite” let us find the most common bigrams not containing stop-words.
:::

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

### Finding Trigam (n = 3) {-}


```{r}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

```

### 4.1.1  Analyzing bigrams {-}

We can use bigrams to do some exploratory analysis. For example, finding the most common strees in each book


```{r}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE) # we used word1, because we have Oxford street always

```
Using `word2`:

```{r}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word2, sort = TRUE) # we used word1, because we have Oxford street always

```

We can use bigram to also find `tf-idf`:




```{r}
bigram_tf_idf <- bigrams_united %>% 
  count(book, bigram) %>% 
   bind_tf_idf(bigram, book, n) %>% 
  arrange(desc(tf_idf))

bigram_tf_idf 

```

:::note
There are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn’t present when one is just counting single words, and may provide context that makes tokens more understandable (for example, “pulteney street”, in Northanger Abbey, is more informative than “pulteney”). However, the per-bigram counts are also sparser: a typical two-word pair is rarer than either of its component words. Thus, bigrams can be especially useful when you have a very large text dataset.
:::

## 4.1.3 Using bigrams to provide context in sentiment analysis {-}


continue from here